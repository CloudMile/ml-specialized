{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## What kind of problem to solve?\n",
    "\n",
    "Music Personalize Recommendation, here because of the target column is binary, we take this as a classification problem, in a nutshell, this is a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "import requests, seaborn as sns\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-whitegrid') # \n",
    "\n",
    "ctx = os.path.abspath(os.path.dirname('.'))\n",
    "if ctx not in sys.path:\n",
    "    sys.path.insert(0, ctx)\n",
    "\n",
    "from trainer import flex, utils, app_conf, metadata, input as inp, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "<br/>\n",
    "\n",
    "### train.csv\n",
    "Column | Description\n",
    "--:--  | --:-- \n",
    "msno | user id\n",
    "song_id | song id\n",
    "source_system_tab | the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search.\n",
    "source_screen_name | name of the layout a user sees.\n",
    "source_type | an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc.\n",
    "target | this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise .\n",
    "<br/>\n",
    "\n",
    "## songs.csv\n",
    "Column | Description\n",
    "--:--  | --:-- \n",
    "song_id | \n",
    "song_length | in ms\n",
    "genre_ids | genre category. Some songs have multiple genres and they are separated by\n",
    "artist_name | \n",
    "composer | \n",
    "lyricist | \n",
    "language |\n",
    "<br/>\n",
    "\n",
    "## members.csv\n",
    "Column | Description\n",
    "--:--  | --:-- \n",
    "msno | \n",
    "city | \n",
    "bd | age. Note: this column has outlier values, please use your judgement.\n",
    "gender | \n",
    "registered_via | registration method\n",
    "registration_init_time | format %Y%m%d\n",
    "expiration_date | format %Y%m%d\n",
    "\n",
    "## song_extra_info.csv\n",
    "Column | Description\n",
    "--:--  | --:-- \n",
    "song_id | \n",
    "song name | the name of the song.\n",
    "isrc | International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>source_system_tab</th>\n",
       "      <th>source_screen_name</th>\n",
       "      <th>source_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=</td>\n",
       "      <td>BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=</td>\n",
       "      <td>explore</td>\n",
       "      <td>Explore</td>\n",
       "      <td>online-playlist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=</td>\n",
       "      <td>3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=</td>\n",
       "      <td>explore</td>\n",
       "      <td>Explore</td>\n",
       "      <td>online-playlist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  \\\n",
       "0  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
       "1  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "2  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "3  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "4  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
       "\n",
       "                                        song_id source_system_tab  \\\n",
       "0  BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=           explore   \n",
       "1  bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=        my library   \n",
       "2  JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=        my library   \n",
       "3  2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=        my library   \n",
       "4  3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=           explore   \n",
       "\n",
       "    source_screen_name      source_type  target  \n",
       "0              Explore  online-playlist       1  \n",
       "1  Local playlist more   local-playlist       1  \n",
       "2  Local playlist more   local-playlist       1  \n",
       "3  Local playlist more   local-playlist       1  \n",
       "4              Explore  online-playlist       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.preview('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>source_system_tab</th>\n",
       "      <th>source_screen_name</th>\n",
       "      <th>source_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=</td>\n",
       "      <td>WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=</td>\n",
       "      <td>y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw=</td>\n",
       "      <td>8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4=</td>\n",
       "      <td>discover</td>\n",
       "      <td>NaN</td>\n",
       "      <td>song-based-playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=</td>\n",
       "      <td>ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ=</td>\n",
       "      <td>radio</td>\n",
       "      <td>Radio</td>\n",
       "      <td>radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=</td>\n",
       "      <td>MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y=</td>\n",
       "      <td>radio</td>\n",
       "      <td>Radio</td>\n",
       "      <td>radio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          msno  \\\n",
       "0   0  V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=   \n",
       "1   1  V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=   \n",
       "2   2  /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw=   \n",
       "3   3  1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=   \n",
       "4   4  1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k=   \n",
       "\n",
       "                                        song_id source_system_tab  \\\n",
       "0  WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc=        my library   \n",
       "1  y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM=        my library   \n",
       "2  8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4=          discover   \n",
       "3  ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ=             radio   \n",
       "4  MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y=             radio   \n",
       "\n",
       "    source_screen_name          source_type  \n",
       "0  Local playlist more        local-library  \n",
       "1  Local playlist more        local-library  \n",
       "2                  NaN  song-based-playlist  \n",
       "3                Radio                radio  \n",
       "4                Radio                radio  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.preview('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "      <th>expiration_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XQxgAYj3klVKjR3oxPPXYYFp4soD4TuBghkhMTD4oTw=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>20110820</td>\n",
       "      <td>20170920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UizsfmJb9mV54qE9hCYyU07Va97c0lCRLEQX3ae+ztM=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>20150628</td>\n",
       "      <td>20170622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D8nEhsIOBSoE6VthTaqDX8U6lqjJ7dLdr72mOyLya2A=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>20160411</td>\n",
       "      <td>20170712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mCuD+tZ1hERA/o5GPqk38e041J8ZsBaLcu7nGoIIvhI=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>20150906</td>\n",
       "      <td>20150907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q4HRBfVSssAFS9iRfxWrohxuk9kCYMKjHOEagUMV6rQ=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>20170126</td>\n",
       "      <td>20170613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  city  bd  gender  \\\n",
       "0  XQxgAYj3klVKjR3oxPPXYYFp4soD4TuBghkhMTD4oTw=     1   0     NaN   \n",
       "1  UizsfmJb9mV54qE9hCYyU07Va97c0lCRLEQX3ae+ztM=     1   0     NaN   \n",
       "2  D8nEhsIOBSoE6VthTaqDX8U6lqjJ7dLdr72mOyLya2A=     1   0     NaN   \n",
       "3  mCuD+tZ1hERA/o5GPqk38e041J8ZsBaLcu7nGoIIvhI=     1   0     NaN   \n",
       "4  q4HRBfVSssAFS9iRfxWrohxuk9kCYMKjHOEagUMV6rQ=     1   0     NaN   \n",
       "\n",
       "   registered_via  registration_init_time  expiration_date  \n",
       "0               7                20110820         20170920  \n",
       "1               7                20150628         20170622  \n",
       "2               4                20160411         20170712  \n",
       "3               9                20150906         20150907  \n",
       "4               4                20170126         20170613  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.preview('./data/members.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_length</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>composer</th>\n",
       "      <th>lyricist</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E=</td>\n",
       "      <td>247640</td>\n",
       "      <td>465</td>\n",
       "      <td>張信哲 (Jeff Chang)</td>\n",
       "      <td>董貞</td>\n",
       "      <td>何啟弘</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU=</td>\n",
       "      <td>197328</td>\n",
       "      <td>444</td>\n",
       "      <td>BLACKPINK</td>\n",
       "      <td>TEDDY|  FUTURE BOUNCE|  Bekuh BOOM</td>\n",
       "      <td>TEDDY</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0=</td>\n",
       "      <td>231781</td>\n",
       "      <td>465</td>\n",
       "      <td>SUPER JUNIOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE=</td>\n",
       "      <td>273554</td>\n",
       "      <td>465</td>\n",
       "      <td>S.H.E</td>\n",
       "      <td>湯小康</td>\n",
       "      <td>徐世珍</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o=</td>\n",
       "      <td>140329</td>\n",
       "      <td>726</td>\n",
       "      <td>貴族精選</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        song_id  song_length  genre_ids  \\\n",
       "0  CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E=       247640        465   \n",
       "1  o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU=       197328        444   \n",
       "2  DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0=       231781        465   \n",
       "3  dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE=       273554        465   \n",
       "4  W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o=       140329        726   \n",
       "\n",
       "        artist_name                            composer     lyricist  language  \n",
       "0  張信哲 (Jeff Chang)                                  董貞          何啟弘       3.0  \n",
       "1         BLACKPINK  TEDDY|  FUTURE BOUNCE|  Bekuh BOOM        TEDDY      31.0  \n",
       "2      SUPER JUNIOR                                 NaN          NaN      31.0  \n",
       "3             S.H.E                                 湯小康          徐世珍       3.0  \n",
       "4              貴族精選                         Traditional  Traditional      52.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.preview('./data/songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>name</th>\n",
       "      <th>isrc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs=</td>\n",
       "      <td>我們</td>\n",
       "      <td>TWUM71200043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE=</td>\n",
       "      <td>Let Me Love You</td>\n",
       "      <td>QMZSY1600015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ=</td>\n",
       "      <td>原諒我</td>\n",
       "      <td>TWA530887303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts=</td>\n",
       "      <td>Classic</td>\n",
       "      <td>USSM11301446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw=</td>\n",
       "      <td>愛投羅網</td>\n",
       "      <td>TWA471306001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        song_id             name          isrc\n",
       "0  LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs=               我們  TWUM71200043\n",
       "1  ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE=  Let Me Love You  QMZSY1600015\n",
       "2  u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ=              原諒我  TWA530887303\n",
       "3  92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts=          Classic  USSM11301446\n",
       "4  0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw=             愛投羅網  TWA471306001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.preview('./data/song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat(multi_col):\n",
    "    \"\"\"Flat the splitted string multivariate feature with the target\n",
    "        - catg1|catg2, 1\n",
    "        - catg2|catg3, 0\n",
    "               to\n",
    "        -   catg1, 1\n",
    "        -   catg2, 0\n",
    "        -   catg2, 0\n",
    "        -   catg3, 0\n",
    "    \"\"\"\n",
    "    multi_list, target = [], []\n",
    "    lens = data[multi_col].map(lambda e: len(e) if isinstance(e, tuple) else 1)\n",
    "    data[multi_col].map(lambda e: multi_list.extend(e) if isinstance(e, tuple) else multi_list.append(None))\n",
    "    pd.Series(list(zip(lens, data.target))).map(lambda tp: target.extend([tp[1]] * tp[0]))\n",
    "    return multi_list, target\n",
    "\n",
    "def univ_boxplot(df, colname):\n",
    "    \"\"\"Draw boxplot to observe the statistic values\"\"\"\n",
    "    anchor_grp = df.groupby(colname)\n",
    "    agg = pd.DataFrame({'mean': anchor_grp.target.mean().drop(''),\n",
    "                        'sum': anchor_grp.target.sum().drop('')})\n",
    "    agg['popular'] = agg['mean'] * agg['sum']\n",
    "    plt.title(f'{colname} Popular Distribution')\n",
    "    ax = sns.boxplot(x=agg.popular)\n",
    "    plt.show()\n",
    "    \n",
    "def multi_catg_heatmap(multi_col, col):\n",
    "    \"\"\"Some features are category string, which contains maybe camma splitted,\n",
    "     means multiple in a grid, called multivariate feature, so this function provide\n",
    "     [univariate feature x multivariate feature] features heatmap with the mean of target\n",
    "    \"\"\"\n",
    "    col_ary, multi_list_ary, target = [], [], []\n",
    "\n",
    "    lens = data[multi_col].map(lambda e: len(e) if isinstance(e, tuple) else 1)\n",
    "    data[multi_col].map(lambda e: multi_list_ary.extend(e) if isinstance(e, tuple) else multi_list_ary.append(None))\n",
    "    pd.Series(list(zip(lens, data[col]))).map(lambda tp: col_ary.extend([tp[1]] * tp[0]))\n",
    "    pd.Series(list(zip(lens, data.target))).map(lambda tp: target.extend([tp[1]] * tp[0]))\n",
    "\n",
    "    df = pd.DataFrame({multi_col: multi_list_ary, col: col_ary, 'target': target})\n",
    "    hm = df.groupby([multi_col, col]).target.mean().reset_index(name='target').pivot(multi_col, col, 'target').fillna(0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(hm)\n",
    "    plt.show()\n",
    "    return hm\n",
    "\n",
    "def heatmap(*cols, annot=True):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    grp = pd.DataFrame({'language': lan, 'city': data.city.values, 'target': data.target.values}).groupby([lan, 'city'])\n",
    "    \n",
    "    pivot_params = list(cols) + ['target']\n",
    "    g = data.groupby(cols).target\n",
    "    mean_ = g.mean().reset_index().pivot(*pivot_params)\n",
    "    count_ = g.size().reset_index().pivot(*pivot_params)\n",
    "    sum_ = g.sum().reset_index().pivot(*pivot_params)\n",
    "    \n",
    "    sns.heatmap( mean_.fillna(0), annot=annot, ax=axs[0] )\n",
    "    sns.heatmap( count_.fillna(0), annot=annot, ax=axs[1] )\n",
    "    sns.heatmap( sum_.fillna(0), annot=annot, ax=axs[2] )\n",
    "    axs[0].set_title('mean')\n",
    "    axs[1].set_title('count')\n",
    "    axs[2].set_title('sum')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean and Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop `name` in `song_extra_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ori_data = pd.read_csv('./data/train.csv')\n",
    "members = pd.read_csv('./data/members.csv')\n",
    "songs = pd.read_csv('./data/songs.csv')\n",
    "song_extra_info = pd.read_csv('./data/song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = ori_data.merge(members, how='left', on='msno')\\\n",
    "               .merge(songs.merge(song_extra_info, how='left', on='song_id'),\\\n",
    "                      how='left', on='song_id')\\\n",
    "               .drop('name', 1)\n",
    "        \n",
    "del members, songs, song_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init logger instance ...\n",
      "init logger instance ...\n"
     ]
    }
   ],
   "source": [
    "try: del data\n",
    "except: pass\n",
    "data = ori_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>song_id</th>\n",
       "      <th>source_system_tab</th>\n",
       "      <th>source_screen_name</th>\n",
       "      <th>source_type</th>\n",
       "      <th>target</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>...</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>composer</th>\n",
       "      <th>lyricist</th>\n",
       "      <th>language</th>\n",
       "      <th>isrc</th>\n",
       "      <th>age</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>artist_list</th>\n",
       "      <th>composer_list</th>\n",
       "      <th>lyricist_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=</td>\n",
       "      <td>BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=</td>\n",
       "      <td>explore</td>\n",
       "      <td>Explore</td>\n",
       "      <td>online-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Bastille</td>\n",
       "      <td>Dan Smith| Mark Crew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>GBUM71602854</td>\n",
       "      <td>outlier</td>\n",
       "      <td>(359,)</td>\n",
       "      <td>(Bastille,)</td>\n",
       "      <td>(Dan Smith, Mark Crew)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Various Artists</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>US3C69910183</td>\n",
       "      <td>20-30</td>\n",
       "      <td>(1259,)</td>\n",
       "      <td>(Various Artists,)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Nas</td>\n",
       "      <td>N. Jones、W. Adams、J. Lordan、D. Ingle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>USUM70618761</td>\n",
       "      <td>20-30</td>\n",
       "      <td>(1259,)</td>\n",
       "      <td>(Nas,)</td>\n",
       "      <td>(N. Jones、W. Adams、J. Lordan、D. Ingle,)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=</td>\n",
       "      <td>2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=</td>\n",
       "      <td>my library</td>\n",
       "      <td>Local playlist more</td>\n",
       "      <td>local-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Soundway</td>\n",
       "      <td>Kwadwo Donkoh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>GBUQH1000063</td>\n",
       "      <td>20-30</td>\n",
       "      <td>(1019,)</td>\n",
       "      <td>(Soundway,)</td>\n",
       "      <td>(Kwadwo Donkoh,)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=</td>\n",
       "      <td>3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=</td>\n",
       "      <td>explore</td>\n",
       "      <td>Explore</td>\n",
       "      <td>online-playlist</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Brett Young</td>\n",
       "      <td>Brett Young| Kelly Archer| Justin Ebach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>QM3E21606003</td>\n",
       "      <td>outlier</td>\n",
       "      <td>(1011,)</td>\n",
       "      <td>(Brett Young,)</td>\n",
       "      <td>(Brett Young, Justin Ebach, Kelly Archer)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  \\\n",
       "0  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
       "1  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "2  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "3  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
       "4  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
       "\n",
       "                                        song_id source_system_tab  \\\n",
       "0  BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=           explore   \n",
       "1  bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=        my library   \n",
       "2  JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=        my library   \n",
       "3  2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=        my library   \n",
       "4  3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=           explore   \n",
       "\n",
       "    source_screen_name      source_type  target  city  bd  gender  \\\n",
       "0              Explore  online-playlist       1     1   0     NaN   \n",
       "1  Local playlist more   local-playlist       1    13  24  female   \n",
       "2  Local playlist more   local-playlist       1    13  24  female   \n",
       "3  Local playlist more   local-playlist       1    13  24  female   \n",
       "4              Explore  online-playlist       1     1   0     NaN   \n",
       "\n",
       "   registered_via      ...           artist_name  \\\n",
       "0               7      ...              Bastille   \n",
       "1               9      ...       Various Artists   \n",
       "2               9      ...                   Nas   \n",
       "3               9      ...              Soundway   \n",
       "4               7      ...           Brett Young   \n",
       "\n",
       "                                  composer  lyricist language          isrc  \\\n",
       "0                     Dan Smith| Mark Crew       NaN     52.0  GBUM71602854   \n",
       "1                                      NaN       NaN     52.0  US3C69910183   \n",
       "2     N. Jones、W. Adams、J. Lordan、D. Ingle       NaN     52.0  USUM70618761   \n",
       "3                            Kwadwo Donkoh       NaN     -1.0  GBUQH1000063   \n",
       "4  Brett Young| Kelly Archer| Justin Ebach       NaN     52.0  QM3E21606003   \n",
       "\n",
       "       age genre_list         artist_list  \\\n",
       "0  outlier     (359,)         (Bastille,)   \n",
       "1    20-30    (1259,)  (Various Artists,)   \n",
       "2    20-30    (1259,)              (Nas,)   \n",
       "3    20-30    (1019,)         (Soundway,)   \n",
       "4  outlier    (1011,)      (Brett Young,)   \n",
       "\n",
       "                               composer_list lyricist_list  \n",
       "0                     (Dan Smith, Mark Crew)           NaN  \n",
       "1                                        NaN           NaN  \n",
       "2    (N. Jones、W. Adams、J. Lordan、D. Ingle,)           NaN  \n",
       "3                           (Kwadwo Donkoh,)           NaN  \n",
       "4  (Brett Young, Justin Ebach, Kelly Archer)           NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binning numeric feature age into categorical feature, \n",
    "# and take [age <= 0 or 81 <= age] as outlier\n",
    "bins = np.array([6, 10, 20, 30, 40, 60, 80])\n",
    "age_map = {0: 'outlier', 1: '6-10', 2: '10-20', 3: '20-30', 4: '30-40', 5: '40-60', 6: '60-80', 7: 'outlier'}\n",
    "data['age'] = pd.Series(np.digitize(data.bd, bins)).map(age_map)\n",
    "data['genre_list'] = data.genre_ids.str.split('\\s*\\|\\s*').map(sorted, na_action='ignore').map(tuple, na_action='ignore')\n",
    "# \n",
    "data['artist_list'] = data.artist_name.str.split('\\s*\\|\\s*').map(sorted, na_action='ignore').map(tuple, na_action='ignore')\n",
    "data['composer_list'] = data.composer.str.split('\\s*\\|\\s*').map(sorted, na_action='ignore').map(tuple, na_action='ignore')\n",
    "data['lyricist_list'] = data.lyricist.str.split('\\s*\\|\\s*').map(sorted, na_action='ignore').map(tuple, na_action='ignore')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init logger instance ...\n"
     ]
    }
   ],
   "source": [
    "enc = utils.CounterEncoder(is_multi=True, sep='|')\n",
    "a = enc.fit_transform(data.genre_ids[:1000])\n",
    "# data.genre_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init logger instance ...\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "pools = multiprocessing.Pool(4)\n",
    "pools.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'list'> to Tensor. Contents: ['359', '1259', '1259', '1019', '1011', '1259', '465', '1011', '2022', '465', '465', '465', '465', '458', '465|458', '465', '465', '458', '465', '465', '458', '465', '465', '2022', '2022', '2022', '465', '465', '465', nan]. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     66\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 67\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got nan",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-61cd28dd9578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# ds = tf.data.Dataset.from_tensor_slices({'col1': [1, 2, 3], 'col2': [(1, 2), (0, 0), (0, 0)]})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'genre_ids'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'list'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \"\"\"\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m   1028\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1029\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m       ])\n\u001b[0;32m   1032\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1028\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1029\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m       ])\n\u001b[0;32m   1032\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m   1012\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                          as_ref=False):\n\u001b[0;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 214\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32md:\\python\\anaconda3\\envs\\py3_6\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    519\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[0;32m    520\u001b[0m                       \u001b[1;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed to convert object of type <class 'list'> to Tensor. Contents: ['359', '1259', '1259', '1019', '1011', '1259', '465', '1011', '2022', '465', '465', '465', '465', '458', '465|458', '465', '465', '458', '465', '465', '458', '465', '465', '2022', '2022', '2022', '465', '465', '465', nan]. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "def map_fn(pipe):\n",
    "    pipe['genre_ids'] = tf.string_split([pipe['genre_ids']], delimiter='|').values\n",
    "    return pipe\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # ds = tf.data.Dataset.from_tensor_slices({'col1': [1, 2, 3], 'col2': [(1, 2), (0, 0), (0, 0)]})\n",
    "    ds = tf.data.Dataset.from_tensor_slices(a.head(30)[['genre_ids', 'target']].to_dict('list'))\n",
    "    ds = ds.map(map_fn)\n",
    "    ds = ds.make_one_shot_iterator().get_next()\n",
    "    # ds = tf.data.Dataset.from_generator(\n",
    "    #     gen_fn, (tf.int32, tf.int32), (tf.TensorShape([None]), tf.TensorShape([]))\n",
    "    # )\n",
    "    with tf.Session() as sess:\n",
    "        print( sess.run(ds) )\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-09 16:23:57,277 - root - ERROR [line:147] - Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = [3, 4, 7]\n",
    "a = pd.Series([1, 2, 3, None, 4, 5, 6]) # .astype(int).map(str).values\n",
    "a.where(a.notna(), [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'composer_list': data.composer_list.dropna().values, \n",
    "                   'len': data.composer_list.dropna().map(len).values})\n",
    "print(df.head())\n",
    "df.loc[df.len.idxmax()].composer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap('language', 'city', annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age and Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_catg_heatmap('genre_list', 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Artist\n",
    "-  We evaluate the popularity of the artist by calculate `mean * sum` by target label, value of target belong to [0, 1], so `sum` can represent the popularity but, maybe there are many 0 in the votes, so multiple the `mean` to get more fair result\n",
    "-  According the boxplot, the popularity distribution is very skew, most of them close to zero, this is expected result, for this situation, we will add some features like the interaction between `[target + artist], [target + language]`..., etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist_list, target = flat('artist_list')\n",
    "univ_boxplot(pd.DataFrame({'artist_list': artist_list, 'target': target}), 'artist_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_list, target = flat('artist_list')\n",
    "univ_boxplot(pd.DataFrame({'artist_list': artist_list, 'target': target}), 'artist_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Filter some rows\n",
    "\n",
    "1. First we filter out the majority and the minority => [100 <= msno.group.size <= 1000], we don't want some majority to dominate the model\n",
    "2. Insure that records of target = 1 greater than 35, we want 30% in valid data records of target = 1 greater than 10\n",
    "    ```\n",
    "    35 * 0.3 = 10.5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Use DataFrameGroupBy.filter will be greate !!!\"\"\"\n",
    "# msno_grp = data.groupby('msno')\n",
    "# msno_grp.size().describe()\n",
    "\n",
    "# df = pd.DataFrame({'size': msno_grp.size(), 'sum': msno_grp.target.sum()})\n",
    "\n",
    "# data = data[data.msno.isin(df.query('sum >= 35 and 100 <= size and size <= 1000').index)]\n",
    "# print(data.shape)\n",
    "# print(data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = utils.CounterEncoder()\n",
    "enc.partial_fit( train.source_system_tab )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([1, 2, 3] / np.linalg.norm([1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'source_system_tab'\n",
    "feat_dummies = pd.get_dummies(data[feat])\n",
    "feat_dummies.columns = ['msno_%s_'%feat + '%s'%col for col in feat_dummies.columns]\n",
    "feat_dummies.head()\n",
    "feat_dummies['msno'] = data['msno'].values\n",
    "feat_dummies = feat_dummies.groupby('msno').mean()\n",
    "feat_dummies['msno'] = feat_dummies.index\n",
    "a = members.merge(feat_dummies, on='msno', how='left')\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Univariate Analysis\n",
    "- Observe some categorical variables, see if there is some sigificant relation\n",
    "- In this section:\n",
    "    - Significant:\n",
    "        - `Promo`: the sales raise in promo perieod.\n",
    "        - `DayOfWeek`: When DayOfWeek = 7, it obviously people count less than average\n",
    "        - `StoreType`: StoreType = 'b' has the lowest counts and the highest average\n",
    "    - Not significant:\n",
    "        - `Promo2`: Nothing found!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Column: Sales\n",
    "1. Observe the the describe table and boxplot of target, the mean is about 5773, so we need to take logrithm in order to reduce scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Sales': tr_merge.Sales.tolist() + np.log1p(tr_merge.Sales).tolist(), \n",
    "                   'clazz': ['Origin'] * len(tr_merge) + ['After np.log1p'] * len(tr_merge)})\n",
    "print(df.query(\"clazz == 'Origin'\").Sales.describe())\n",
    "print()\n",
    "print(df.query(\"clazz == 'After np.log1p'\").Sales.describe())\n",
    "sns.factorplot(x='clazz', y='Sales', data=df, kind=\"box\", size=4, aspect=2, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "print(tr_merge.groupby('Promo').Sales.sum())\n",
    "g = sns.FacetGrid(tr_merge, row='Open', col='Promo', size=2, aspect=2)\n",
    "g = g.map(plt.hist, \"Sales\").add_legend()\n",
    "\n",
    "sns.factorplot(x=\"Promo\", y=\"Sales\", data=tr_merge, kind=\"box\", size=4, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tr_merge.groupby(['Promo']).Sales.describe()\n",
    "agg['sum'] = agg['count'] * agg['mean']\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "print(tr_merge.groupby('promo2').Sales.sum())\n",
    "g = sns.FacetGrid(tr_merge, row='Open', col='promo2', size=2, aspect=2)\n",
    "g = g.map(plt.hist, \"Sales\").add_legend()\n",
    "\n",
    "sns.factorplot(x=\"promo2\", y=\"Sales\", data=tr_merge, kind=\"box\", size=4, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tr_merge.groupby(['promo2']).Sales.describe()\n",
    "agg['sum'] = agg['count'] * agg['mean']\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "g = sns.FacetGrid(tr_merge, col='DayOfWeek', sharey=True, size=3, aspect=1)\n",
    "g = g.map(plt.hist, \"Sales\")\n",
    "\n",
    "sns.factorplot(x=\"DayOfWeek\", y=\"Sales\", data=tr_merge, kind=\"box\", size=4, aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tr_merge.groupby('DayOfWeek').Sales.describe()\n",
    "agg['sum'] = agg['count'] * agg['mean']\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StoreType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(tr_merge, col='StoreType', sharey=True, size=3, aspect=1)\n",
    "g = g.map(plt.hist, \"Sales\")\n",
    "\n",
    "sns.factorplot(x=\"StoreType\", y=\"Sales\", data=tr_merge, kind=\"box\", size=4, aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tr_merge.groupby('StoreType').Sales.describe()\n",
    "agg['sum'] = agg['count'] * agg['mean']\n",
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Bivariate Analysis\n",
    "- In this section:\n",
    "    - Significant:\n",
    "        - `Promo + StateHoliday`: In mean heatmap figure, there is a peak on StateHoliday = 'b'\n",
    "        - `State + StoreType`: When StoreType = 'b', the average of sales are unstable, and many States don't have StoreType 'b'\n",
    "        \n",
    "    - Not significant:\n",
    "        - `Promo + SchoolHoliday`: Just found sum of the records with SchoolHoliday = 1 are less\n",
    "        - `month + day`: In count heatmap, obviously records count(customers count) before july are upper than average, but we founds many store loss records in late date of timeline, that's why\n",
    "        - `year + month`: Some peak in mean heatmap in December, corresponding to mean heatmap of [month + day], not really significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(*cols, annot=True):\n",
    "    # cols = ['Promo', 'SchoolHoliday']\n",
    "    pivot_params = list(cols) + ['Sales']\n",
    "    g = tr_merge.groupby(cols).Sales\n",
    "    mean_ = g.mean().reset_index().pivot(*pivot_params)\n",
    "    count_ = g.size().reset_index().pivot(*pivot_params)\n",
    "    sum_ = g.sum().reset_index().pivot(*pivot_params)\n",
    "\n",
    "    f, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    sns.heatmap(mean_, annot=annot, ax=axs[0])\n",
    "    sns.heatmap(count_, annot=annot, ax=axs[1])\n",
    "    sns.heatmap(sum_, annot=annot, ax=axs[2])\n",
    "    axs[0].set_title(f'mean')\n",
    "    axs[1].set_title(f'count')\n",
    "    axs[2].set_title(f'sum')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promo, SchoolHoliday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tr_merge.groupby(['Store', 'SchoolHoliday', 'Promo']).Sales.sum())\n",
    "g = sns.FacetGrid(tr_merge, row='SchoolHoliday', col='Promo', size=2, aspect=2)\n",
    "g = g.map(plt.hist, \"Sales\").add_legend()\n",
    "\n",
    "sns.factorplot(x=\"Promo\", y=\"Sales\", col='SchoolHoliday', data=tr_merge, kind=\"box\", size=3, aspect=2)\n",
    "\n",
    "heatmap('Promo', 'SchoolHoliday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promo, StateHoliday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(tr_merge, row='StateHoliday', col='Promo', size=2, aspect=2)\n",
    "# g = g.map(plt.hist, \"Sales\").add_legend()\n",
    "\n",
    "sns.factorplot(x=\"Promo\", y=\"Sales\", col='StateHoliday', data=tr_merge, kind=\"box\", size=3, aspect=2)\n",
    "\n",
    "heatmap('Promo', 'StateHoliday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State, StoreType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(tr_merge, col='State', row='StoreType', sharey=True, size=4, aspect=.5)\n",
    "# g = g.map(plt.hist, \"Sales\")\n",
    "\n",
    "sns.factorplot(x=\"State\", y=\"Sales\", row='StoreType', data=tr_merge, kind=\"box\", size=4, aspect=3)\n",
    "\n",
    "heatmap('State', 'StoreType', annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.factorplot(x=\"day\", y=\"Sales\", row='month', data=tr_merge, kind=\"box\", size=4, aspect=3)\n",
    "heatmap('month', 'day', annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Recall The Exploration\n",
    "There are many combination of columns we didn't see, to view all the combination is not efficient, that's why we use embedding to encode the categorical variables, in Tensorflow, `tf.feature_columns.crossed_column` can help us to make machine learn the information hidden in data, or handle all the combination by yourself, but imagine that we couldn't expect all the crossed combination happend in training data, if some outlier happened in serving time, the performance usually bad as expected.\n",
    "<br/><br/>\n",
    "Fortunately, without using crossed features we still got good performance, and we drop `year` columns as the same reason we mentioned, future year not show in training data result in the bad performance happended (Remember we treat year as categorical variable), month and day features  show periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module structure\n",
    "    ```\n",
    "    └─trainer\n",
    "        │  app_conf.py\n",
    "        │  ctrl.py\n",
    "        │  input.py\n",
    "        │  logging.yaml\n",
    "        │  metadata.py\n",
    "        │  model.py\n",
    "        │  service.py\n",
    "        └─ utils.py        \n",
    "    ```\n",
    "    Directory trainer contains all modules: \n",
    "    \n",
    "|File Name| Purpose| Do You Need to Change?\n",
    "|:---|:---|:---\n",
    "|[app_conf.py](trainer/app_conf.py) | Applicaiton configures, tell where are the training data, the model checkpoint directory, and hyperparameter | **Yes**, any params you don't want to pass in ctrl.py, you can put them here, but still, like the path to train directory, it's suggested to put app_conf.py instead of pass on the fly to controller.\n",
    "|[ctrl.py](trainer/ctrl.py) |The entrance of the program, accept any parameters, protable for local terminal or restful style environment. | **Maybe**, althought the processing is constructed, some of detail you might wont to modify.\n",
    "|[service.py](trainer/service.py) |The business logic module behind the controller(ctrl.py) | **Yes**, usually you still have your own programe process\n",
    "|[utils.py](trainer/service.py) | Utility module, some common function to put here, like logging function. | **Yes**, maybe you will have some special utils function to implement.\n",
    "|[metadata.py](trainer/metadata.py)|Defines: 1) task type, 2) input data header, 3) numeric and categorical feature names, 4) target feature name (and labels, for a classification task), and 5) unused feature names. | **Yes**, as you will need to specify the metadata of your dataset. **This might be the only module to change!**\n",
    "|[input.py](trainer/input.py)| Includes: 1) data input functions to read data from csv and tfrecords files, 2) parsing functions to convert csv and tf.example to tensors, 3) function to implement your features custom  processing and creation functionality, and 4) prediction functions (for serving the model) that accepts CSV, JSON, and tf.example instances. | **Maybe**, if you want to implement any custom pre-processing and feature creation during reading data.\n",
    "|[model.py](trainer/model.py)|Includes: 1) function to create DNNRegressor, 2) function to implement for a custom estimator model_fn. 3) include **Feature class** to assign the feature spec, the feature spec usually bind for specific model, so we put them together. | **Yes**, in **Model.get_estimator** Usually, at least you want to modify the hidden_units params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Data preprocessing\n",
    "1. Data offered like this\n",
    "    ```\n",
    "    │  store.csv\n",
    "    │  store_states.csv\n",
    "    │  test.csv\n",
    "    └─ train.csv\n",
    "    ```\n",
    "2. Data pipeline on training period we propose 4 steps: **prepare -> fit -> transform -> split**\n",
    "    - Prepare: \n",
    "        - Join store and store_states to make the **Fat table**\n",
    "        - Add features we mentioned in data exploration, drop also.\n",
    "        - Filter some records not appropriate, like open = 0\n",
    "        - Maybe persistent some files\n",
    "    - Fit: \n",
    "        - Persistent the statistical information of numeric features\n",
    "        - Persistent the unique count value of categorical features\n",
    "    - Transform:\n",
    "        - Normalization, logarithm ... etc.\n",
    "        - Make all categorical variable to int, one hot encoding ... etc.\n",
    "        - Take logarithm of the target column\n",
    "    - Split:\n",
    "        - Do split logic, in this case we order by date, and take 0.3 as valid data size, train data is first 0.7 part, and rest is valid data\n",
    "        \n",
    "3. Data pipeline on serving period just 2 steps: **prepare -> transform**\n",
    "    - Prepare: As prepare step in training period, just some detail different, like not filter open = 0 here\n",
    "    - Transform: Same as transform step in training period, usually delete the target column\n",
    "    \n",
    "4. There is something weird is feature `Customers` is not in the test.csv, at heatmap of pearson corr we know that this feature got linear relation with the target column, but pity is we have to drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, input, metadata\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "Ctrl.instance.prepare(app_conf.instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Model Structure\n",
    "\n",
    "The model mainly use tf.esimator.DNNRegressor\n",
    "- Loss function: mean squre error, default fixed by `tf.estimator.DNNRegressor` \n",
    "- DNN Structure: `81 -> 1000 -> 500 -> 1`, 81 is the dimension of concatenate all features, the last output dimension must set to 1 because of the prediction target is float.\n",
    "- No dropout: check the train section, we use best checkpoint picker to avoid overfitting.\n",
    "- ReLU activation function at each layer\n",
    "<br/><br/>\n",
    "\n",
    "## Train\n",
    "- Training for 3 epochs, use `tf.estimator.DNNRegressor`\n",
    "- Use `tf.train.GradientDescent` Optimizer with learning rate `0.0001`\n",
    "- Batch size = 256\n",
    "- Best exporter: we do evaluation on every train epoch end, the export condition is better than last evaluation(lower loss or higher accuracy),\n",
    "we don't export the worse result.\n",
    "- When validation time, we observe the `MAE(Mean Absolute Error)` and `RMSE(Root Mean Square Error)` error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, metadata, input as inp, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "ctrl = Ctrl.instance\n",
    "ctrl.train(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance(TensorBoard Screen Shot): RMSE on Valid Data: 0.19\n",
    "![Pic from tensrorboard](model_metric.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol buffer file inspection\n",
    "- Use saved_model_cli to inspect the protocol buff schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = utils.find_latest_expdir(app_conf.instance)\n",
    "print( utils.cmd(f'saved_model_cli show --dir {export_dir}') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export_dir = utils.find_latest_expdir(app_conf.instance)\n",
    "print( utils.cmd(f'saved_model_cli show --dir {export_dir} --tag_set serve') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export_dir = utils.find_latest_expdir(app_conf.instance)\n",
    "command = f'saved_model_cli show --dir {export_dir} --tag_set serve --signature_def predict'\n",
    "print( utils.cmd(command) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local prediction\n",
    "1. Transform raw input data for prediction\n",
    "2. Local prediction\n",
    "    - call ctrl.local_predict: simply use `tensorflow.contrib.predictor`\n",
    "        ```python\n",
    "        from tensorflow.contrib import predictor\n",
    "        predict_fn = predictor.from_saved_model(export_dir, signature_def_key='predict')\n",
    "        pred = predict_fn( ... )\n",
    "        ```\n",
    "3. Online prediciton\n",
    "    - deploy model to ml-engine\n",
    "        1. Create model reository(resource)\n",
    "        2. Add version in model container, in this example we clean the reository first (delete all old version)\n",
    "        3. Determine the **default version**, default is the latest version\n",
    "    - call ctrl.online_predict\n",
    "    \n",
    "4. In this case, we need to take np.expm1 to the predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, metadata, input, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "ctrl = Ctrl.instance\n",
    "test_df = ctrl.transform(pd.Series({'fpath': './data/test.csv'}))\n",
    "params = pd.Series({\n",
    "    'datasource': test_df,\n",
    "    'is_src_file': False\n",
    "})\n",
    "pred = Ctrl.instance.local_predict(params)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deply to ml-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set environ variable to read the api key json file: \n",
    "    - In trainer/app_conf.py, `Config.api_key_path` set the API key json path to acquire the access to any GCP service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from trainer import app_conf, metadata, input, service\n",
    "from trainer.ctrl import Ctrl\n",
    "ctrl = Ctrl.instance\n",
    "\n",
    "ctrl.set_client_secret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload saved pb file to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ctrl = Ctrl.instance\n",
    "p = pd.Series({\n",
    "    'bucket_name': 'ml-specialized',\n",
    "    'model_path': utils.find_latest_expdir(app_conf.instance),\n",
    "    'prefix': 'models/rossmann'\n",
    "})\n",
    "ctrl.upload_model(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy\n",
    "- Create model resource and version\n",
    "- Call deploy api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = Ctrl.instance\n",
    "p = pd.Series({\n",
    "    'model_name': 'ml_specilized_rossmann',\n",
    "    'deployment_uri': 'gs://ml-specialized/models/rossmann'\n",
    "})\n",
    "ctrl.deploy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call deployed model\n",
    "\n",
    "Because of the traffic volume limitation, just show first 100 rows online prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = Ctrl.instance.transform(pd.Series({'fpath': './data/test.csv'}))\n",
    "p = pd.Series({\n",
    "    'model_name': 'ml_specilized_rossmann',\n",
    "    'datasource': test_df[:100].to_dict('records')\n",
    "})\n",
    "result = Ctrl.instance.online_predict(p)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Check predictions\n",
    "---\n",
    "Plot timeline in each store sales, prev 70% are training data and post 30% are validation data, predict valid data and compare to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./data/processed/train_pr.csv').query('open == 1')\n",
    "vl = pd.read_csv('./data/processed/valid_pr.csv').query('open == 1')\n",
    "# Add date column for time series inspection\n",
    "tr['date'] = pd.read_json(app_conf.instance.tr_dt_file, typ='series').values\n",
    "vl['date'] = pd.read_json(app_conf.instance.vl_dt_file, typ='series').values\n",
    "\n",
    "tr['predict'] = np.nan\n",
    "params = pd.Series({\n",
    "    'datasource': vl.drop(['sales', 'date'], 1),\n",
    "    'is_src_file': False\n",
    "})\n",
    "vl['predict'] = Ctrl.instance.local_predict(params)\n",
    "\n",
    "merge = pd.concat([tr, vl], 0)\n",
    "merge['sales'] = np.expm1(merge.sales)\n",
    "merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modify the `pos` variable to controll the start position, the following plot will show 3 figures a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_rows, n_cols = 3, 1\n",
    "f, ax = plt.subplots(n_rows, n_cols, figsize=(20, 8), sharex=True, sharey=True)\n",
    "for r, idx in enumerate(np.arange(pos, pos + n_rows)):\n",
    "    samples = merge.query(f'store == {idx}').sort_values('date')\n",
    "    hist = samples.query('predict.isnull()')\n",
    "    predict = samples.query('predict.notnull()')\n",
    "    ax[r].plot(np.arange(len(hist)), hist.sales.values, c='gray', label='history')\n",
    "    ax[r].plot(np.arange(len(hist), len(hist) + len(predict)), predict.sales.values, label='actual')\n",
    "    ax[r].plot(np.arange(len(hist), len(hist) + len(predict)), predict.predict.values, label='predict')\n",
    "    ax[r].set_title(f'{idx}', size=18)\n",
    "    ax[r].grid(True)\n",
    "    ax[r].legend(loc='best', prop={'size': 16})\n",
    "pos += n_rows\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use `tf.saved_model.loader.load`, only for `json_serving_input_fn `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, metadata, input, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "ctrl = Ctrl.instance\n",
    "test_df = ctrl.transform(pd.Series({'fpath': './data/test.csv'}))\n",
    "params = pd.Series({\n",
    "    'datasource': test_df,\n",
    "    'is_src_file': False\n",
    "})\n",
    "pred = ctrl.local_predict_alt(params)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, metadata, input, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "encoded, origin, feat_data, target_, all_ = Ctrl.instance.inspect('promo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from trainer import app_conf, metadata, input, service\n",
    "from trainer.ctrl import Ctrl\n",
    "\n",
    "ctrl = Ctrl.instance\n",
    "serv_inp = ctrl.test()\n",
    "pprint(serv_inp.receiver_tensors)\n",
    "print()\n",
    "pprint(serv_inp.features)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
